{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyRNNsearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhG2TWRlCyjWSNd5md9sw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/MyRNNsearch/blob/main/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp7K39N2_tU0"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This project concerns itself with a Natural Language Processing (\"NLP\") system which is able to translate a given piece of text from one language into another. It is part of the examination in the lecture on \"Deep Learning for Natural Language Processing\" of the M. Sc. Cognitive Systems at the University of Potsdam.\n",
        "\n",
        "It is based on the paper:\n",
        "\n",
        "\n",
        "> Bahdanau, Cho & Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015.\n",
        "\n",
        "\n",
        "The task is to implement the RNNsearch-50 system, i.e., the encoder-decoder with attention system for any language pair different from English-German, German-English, English-French, and French-English.\n",
        "\n",
        "The experiments were conducted in the form of a Jupyter Notebook to provide commentary on the underlying thought process and in order to have GPU access through Google Colaboratory. However, this notebook is non-exhaustive in its documentation. An in-depth walk-through of the project is given in the accompanying report which can be found here: **add link**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhVOlKEwH9Ez"
      },
      "source": [
        "# Setup\n",
        "\n",
        "If you choose to access the project through the related Jupyter Notebook it will initially clone the python files from the research repository and install any dependency packages. These dependencies include the TensorFlow dataset and SpaCy pipelines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz_I8H-r3VtB",
        "outputId": "d18d34a6-ffd7-4801-fa15-e7b9a67d0fdd"
      },
      "source": [
        "!git clone -l -s https://github.com/Dagobert42/MyRNNsearch.git temp\n",
        "%cd temp\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'temp'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 8 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n",
            "/content/temp\n",
            "config.yml  model.py  README.md  run.py  tests.py  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5O7YppZW29I"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41pEJQYt_fGa"
      },
      "source": [
        "!pip install --upgrade tensorflow-datasets\n",
        "time.sleep(5)\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBUzcOj_mRl"
      },
      "source": [
        "# Data\n",
        "\n",
        "The chosen language pair English-Danish. A dataset of matched sentence pairs for these languages can be obtained at runtime via a TensorFlow ParaCrawl configuration [cite]. We propose this dataset because it is large enough (2.414.895 examples, 362.46 MiB) to allow the model to learn in a meaningful way, while being small enough to be loaded into memory. Furthermore, pretrained SpaCy pipelines exist for both of these languages, providing us with tools for tokenization as well as sets of 30.000 ready-to-use word vectors for each language. We disable any other parts of the SpaCy pipelines during loading as they are not relevant for the project. Pre-existing TensorFlow ParaCrawl configurations also include various other target languages for translation from English (insert link). Hypothetically, it should be possible to run the experiments with any one of them (although some of the datasets are exceedingly large and it might not be possible to load them at runtime). Also please keep in mind that there may not be a SpaCy pipeline for the desired target language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4k3iFo_Wf5y"
      },
      "source": [
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "!python -m spacy download da_core_news_md\n",
        "time.sleep(5)\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7E8p_kDYxEh"
      },
      "source": [
        "import data\n",
        "# get builder for English-Danish\n",
        "builder = get_dataset_builder(target_language='da')\n",
        "builder.download_and_prepare()\n",
        "train, test, val = get_data_splits(builder)\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Off99dUMHI3-"
      },
      "source": [
        "We load the SpaCy pipelines. SpaCy provides us with an alternative way to obtain tokens for each language. Taking our handmade vocabulary as a baseline we can observe differences in performance that SpaCy might provide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpAesFhDAOc2"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.examples import sentences as e\n",
        "from spacy.lang.da.examples import sentences as d\n",
        "\n",
        "english_nlp = spacy.load(\"en_core_news_md\", exclude=[\"tagger\", \"parser\", \"senter\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])\n",
        "danish_nlp = spacy.load(\"da_core_news_md\", exclude=[\"morphologizer\", \"parser\", \"senter\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-dyp-N_p9Z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GANUhExt_1Rt"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv-OqhNE_4Et"
      },
      "source": [
        "# Evaluation"
      ]
    }
  ]
}