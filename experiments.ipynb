{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyRNNsearch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgO6yXuXfaDnqMD35fawAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/MyRNNsearch/blob/main/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp7K39N2_tU0"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This project concerns itself with a Natural Language Processing (\"NLP\") system which is able to translate a given piece of text from one language into another. It is part of the examination in the lecture on \"Deep Learning for Natural Language Processing\" of the M. Sc. Cognitive Systems at the University of Potsdam.\n",
        "\n",
        "It is based on the paper:\n",
        "\n",
        "\n",
        "> Bahdanau, Cho & Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015.\n",
        "\n",
        "\n",
        "The task is to implement the RNNsearch-50 system, i.e., the encoder-decoder with attention system for any language pair different from English-German, German-English, English-French, and French-English.\n",
        "\n",
        "This notebook is non-exhaustive in its documentation. An in-depth walk-through of the project is given in the accompanying report which can be found here: **add link**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhVOlKEwH9Ez"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Please choose which dependencies to install on your machine. To save time you can omit these on re-runs by unchecking the boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz_I8H-r3VtB"
      },
      "source": [
        "#@title Install Dependencies\n",
        "\n",
        "clone_repo = False #@param {type:\"boolean\"}\n",
        "if clone_repo:\n",
        "    !git clone -l -s https://github.com/Dagobert42/MyRNNsearch.git\n",
        "    !ls\n",
        "\n",
        "install_tf_datasets = False #@param {type:\"boolean\"}\n",
        "if install_tf_datasets:\n",
        "    !pip install --upgrade tensorflow-datasets\n",
        "\n",
        "install_spacy_packs = False #@param {type:\"boolean\"}\n",
        "if install_spacy_packs:\n",
        "    !pip install --upgrade spacy\n",
        "    !python -m spacy download en_core_web_md\n",
        "    !python -m spacy download da_core_news_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBUzcOj_mRl"
      },
      "source": [
        "# Data\n",
        "\n",
        "Choose a target language for the ParaCrawl configuaration. ParaCrawl provides a number of English-*Target* datasets. Beware that SpaCy pipelines will have to be downloaded and configured manually for languages other than Danish. See this link for a comprehensive list: https://www.tensorflow.org/datasets/catalog/para_crawl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7E8p_kDYxEh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "24725fe5-6a40-4596-b91b-19b1a46a293f"
      },
      "source": [
        "#@title Target Language\n",
        "\n",
        "import data\n",
        "\n",
        "language = 'da' #@param ['da', 'cs', 'el', 'et', 'fi', 'ga', 'hr', 'hu', 'de', 'it', 'es']\n",
        "builder = data.get_para_crawl_builder(language)\n",
        "builder.download_and_prepare()\n",
        "\n",
        "train, test, val = data.get_data_splits(builder)\n",
        "train[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a6b582c6f06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Target Language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'da'\u001b[0m \u001b[0;31m#@param ['da', 'cs', 'el', 'et', 'fi', 'ga', 'hr', 'hu', 'de', 'it', 'es']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Off99dUMHI3-"
      },
      "source": [
        "SpaCy provides us with an alternative way to obtain tokens for each language. Taking our handmade vocabulary as a baseline we can observe differences in performance that SpaCy might provide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpAesFhDAOc2"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.examples import sentences as e\n",
        "from spacy.lang.da.examples import sentences as d\n",
        "\n",
        "english_nlp = spacy.load(\"en_core_news_sm\", exclude=[\"tagger\", \"parser\", \"senter\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])\n",
        "danish_nlp = spacy.load(\"da_core_news_sm\", exclude=[\"morphologizer\", \"parser\", \"senter\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-dyp-N_p9Z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GANUhExt_1Rt"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv-OqhNE_4Et"
      },
      "source": [
        "# Evaluation"
      ]
    }
  ]
}